{
  "best_metric": 0.9533,
  "best_model_checkpoint": "test_trainer\\checkpoint-3375",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 3375,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.008888888888888889,
      "grad_norm": 4.312137603759766,
      "learning_rate": 1.4792899408284024e-06,
      "loss": 0.7713,
      "step": 10
    },
    {
      "epoch": 0.017777777777777778,
      "grad_norm": 4.900791168212891,
      "learning_rate": 2.9585798816568047e-06,
      "loss": 0.7396,
      "step": 20
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 15.371414184570312,
      "learning_rate": 4.437869822485207e-06,
      "loss": 0.7045,
      "step": 30
    },
    {
      "epoch": 0.035555555555555556,
      "grad_norm": 5.09067440032959,
      "learning_rate": 5.917159763313609e-06,
      "loss": 0.6585,
      "step": 40
    },
    {
      "epoch": 0.044444444444444446,
      "grad_norm": 4.224398612976074,
      "learning_rate": 7.396449704142013e-06,
      "loss": 0.6245,
      "step": 50
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 3.632559299468994,
      "learning_rate": 8.875739644970414e-06,
      "loss": 0.5918,
      "step": 60
    },
    {
      "epoch": 0.06222222222222222,
      "grad_norm": 3.536371946334839,
      "learning_rate": 1.0355029585798817e-05,
      "loss": 0.6365,
      "step": 70
    },
    {
      "epoch": 0.07111111111111111,
      "grad_norm": 3.839207172393799,
      "learning_rate": 1.1834319526627219e-05,
      "loss": 0.5676,
      "step": 80
    },
    {
      "epoch": 0.08,
      "grad_norm": 3.7726423740386963,
      "learning_rate": 1.3313609467455624e-05,
      "loss": 0.5414,
      "step": 90
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 10.602768898010254,
      "learning_rate": 1.4792899408284025e-05,
      "loss": 0.5451,
      "step": 100
    },
    {
      "epoch": 0.09777777777777778,
      "grad_norm": 7.012412071228027,
      "learning_rate": 1.6272189349112425e-05,
      "loss": 0.5286,
      "step": 110
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 8.084014892578125,
      "learning_rate": 1.7751479289940828e-05,
      "loss": 0.5094,
      "step": 120
    },
    {
      "epoch": 0.11555555555555555,
      "grad_norm": 7.841710090637207,
      "learning_rate": 1.923076923076923e-05,
      "loss": 0.4771,
      "step": 130
    },
    {
      "epoch": 0.12444444444444444,
      "grad_norm": 10.119993209838867,
      "learning_rate": 2.0710059171597635e-05,
      "loss": 0.5118,
      "step": 140
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 5.300312519073486,
      "learning_rate": 2.2189349112426034e-05,
      "loss": 0.493,
      "step": 150
    },
    {
      "epoch": 0.14222222222222222,
      "grad_norm": 6.185497760772705,
      "learning_rate": 2.3668639053254438e-05,
      "loss": 0.4851,
      "step": 160
    },
    {
      "epoch": 0.1511111111111111,
      "grad_norm": 10.08010482788086,
      "learning_rate": 2.514792899408284e-05,
      "loss": 0.4365,
      "step": 170
    },
    {
      "epoch": 0.16,
      "grad_norm": 8.884448051452637,
      "learning_rate": 2.6627218934911247e-05,
      "loss": 0.4087,
      "step": 180
    },
    {
      "epoch": 0.1688888888888889,
      "grad_norm": 7.5336713790893555,
      "learning_rate": 2.8106508875739644e-05,
      "loss": 0.4489,
      "step": 190
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 9.035347938537598,
      "learning_rate": 2.958579881656805e-05,
      "loss": 0.409,
      "step": 200
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 8.482542037963867,
      "learning_rate": 3.106508875739645e-05,
      "loss": 0.4135,
      "step": 210
    },
    {
      "epoch": 0.19555555555555557,
      "grad_norm": 19.214244842529297,
      "learning_rate": 3.254437869822485e-05,
      "loss": 0.4071,
      "step": 220
    },
    {
      "epoch": 0.20444444444444446,
      "grad_norm": 7.361783981323242,
      "learning_rate": 3.402366863905326e-05,
      "loss": 0.3502,
      "step": 230
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 12.169071197509766,
      "learning_rate": 3.5502958579881656e-05,
      "loss": 0.3601,
      "step": 240
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 12.148900032043457,
      "learning_rate": 3.698224852071006e-05,
      "loss": 0.3588,
      "step": 250
    },
    {
      "epoch": 0.2311111111111111,
      "grad_norm": 11.805462837219238,
      "learning_rate": 3.846153846153846e-05,
      "loss": 0.4121,
      "step": 260
    },
    {
      "epoch": 0.24,
      "grad_norm": 21.015005111694336,
      "learning_rate": 3.9940828402366866e-05,
      "loss": 0.3914,
      "step": 270
    },
    {
      "epoch": 0.24888888888888888,
      "grad_norm": 8.015970230102539,
      "learning_rate": 4.142011834319527e-05,
      "loss": 0.4092,
      "step": 280
    },
    {
      "epoch": 0.2577777777777778,
      "grad_norm": 17.938581466674805,
      "learning_rate": 4.289940828402367e-05,
      "loss": 0.3601,
      "step": 290
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 6.3870954513549805,
      "learning_rate": 4.437869822485207e-05,
      "loss": 0.3667,
      "step": 300
    },
    {
      "epoch": 0.27555555555555555,
      "grad_norm": 7.02772855758667,
      "learning_rate": 4.585798816568048e-05,
      "loss": 0.3438,
      "step": 310
    },
    {
      "epoch": 0.28444444444444444,
      "grad_norm": 12.738110542297363,
      "learning_rate": 4.7337278106508875e-05,
      "loss": 0.3809,
      "step": 320
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 7.664763927459717,
      "learning_rate": 4.881656804733728e-05,
      "loss": 0.3505,
      "step": 330
    },
    {
      "epoch": 0.3022222222222222,
      "grad_norm": 11.257596015930176,
      "learning_rate": 4.996707276918011e-05,
      "loss": 0.3114,
      "step": 340
    },
    {
      "epoch": 0.3111111111111111,
      "grad_norm": 13.374557495117188,
      "learning_rate": 4.9802436615080674e-05,
      "loss": 0.3583,
      "step": 350
    },
    {
      "epoch": 0.32,
      "grad_norm": 19.476219177246094,
      "learning_rate": 4.963780046098123e-05,
      "loss": 0.377,
      "step": 360
    },
    {
      "epoch": 0.3288888888888889,
      "grad_norm": 13.317275047302246,
      "learning_rate": 4.947316430688179e-05,
      "loss": 0.3339,
      "step": 370
    },
    {
      "epoch": 0.3377777777777778,
      "grad_norm": 8.10871410369873,
      "learning_rate": 4.930852815278235e-05,
      "loss": 0.3425,
      "step": 380
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 15.885085105895996,
      "learning_rate": 4.914389199868291e-05,
      "loss": 0.3392,
      "step": 390
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 11.084595680236816,
      "learning_rate": 4.8979255844583474e-05,
      "loss": 0.3019,
      "step": 400
    },
    {
      "epoch": 0.36444444444444446,
      "grad_norm": 13.952971458435059,
      "learning_rate": 4.881461969048403e-05,
      "loss": 0.2939,
      "step": 410
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 5.570925712585449,
      "learning_rate": 4.864998353638459e-05,
      "loss": 0.3167,
      "step": 420
    },
    {
      "epoch": 0.38222222222222224,
      "grad_norm": 6.702107906341553,
      "learning_rate": 4.848534738228515e-05,
      "loss": 0.2885,
      "step": 430
    },
    {
      "epoch": 0.39111111111111113,
      "grad_norm": 9.266282081604004,
      "learning_rate": 4.832071122818571e-05,
      "loss": 0.3499,
      "step": 440
    },
    {
      "epoch": 0.4,
      "grad_norm": 9.891498565673828,
      "learning_rate": 4.815607507408627e-05,
      "loss": 0.3203,
      "step": 450
    },
    {
      "epoch": 0.4088888888888889,
      "grad_norm": 8.582230567932129,
      "learning_rate": 4.799143891998683e-05,
      "loss": 0.3502,
      "step": 460
    },
    {
      "epoch": 0.4177777777777778,
      "grad_norm": 6.074405670166016,
      "learning_rate": 4.7826802765887394e-05,
      "loss": 0.3098,
      "step": 470
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 104.93709564208984,
      "learning_rate": 4.766216661178795e-05,
      "loss": 0.2953,
      "step": 480
    },
    {
      "epoch": 0.43555555555555553,
      "grad_norm": 5.676672458648682,
      "learning_rate": 4.749753045768851e-05,
      "loss": 0.2871,
      "step": 490
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 9.871156692504883,
      "learning_rate": 4.733289430358907e-05,
      "loss": 0.3176,
      "step": 500
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 17.948970794677734,
      "learning_rate": 4.716825814948963e-05,
      "loss": 0.3018,
      "step": 510
    },
    {
      "epoch": 0.4622222222222222,
      "grad_norm": 9.377435684204102,
      "learning_rate": 4.700362199539019e-05,
      "loss": 0.3035,
      "step": 520
    },
    {
      "epoch": 0.4711111111111111,
      "grad_norm": 5.090084552764893,
      "learning_rate": 4.683898584129075e-05,
      "loss": 0.2678,
      "step": 530
    },
    {
      "epoch": 0.48,
      "grad_norm": 4.949427604675293,
      "learning_rate": 4.667434968719131e-05,
      "loss": 0.2626,
      "step": 540
    },
    {
      "epoch": 0.4888888888888889,
      "grad_norm": 5.391843318939209,
      "learning_rate": 4.650971353309187e-05,
      "loss": 0.267,
      "step": 550
    },
    {
      "epoch": 0.49777777777777776,
      "grad_norm": 6.584529876708984,
      "learning_rate": 4.634507737899243e-05,
      "loss": 0.3337,
      "step": 560
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 9.5055513381958,
      "learning_rate": 4.618044122489299e-05,
      "loss": 0.2736,
      "step": 570
    },
    {
      "epoch": 0.5155555555555555,
      "grad_norm": 9.034135818481445,
      "learning_rate": 4.601580507079355e-05,
      "loss": 0.3535,
      "step": 580
    },
    {
      "epoch": 0.5244444444444445,
      "grad_norm": 5.378804683685303,
      "learning_rate": 4.585116891669411e-05,
      "loss": 0.2956,
      "step": 590
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 9.241389274597168,
      "learning_rate": 4.568653276259467e-05,
      "loss": 0.2791,
      "step": 600
    },
    {
      "epoch": 0.5422222222222223,
      "grad_norm": 7.627205848693848,
      "learning_rate": 4.5521896608495226e-05,
      "loss": 0.2916,
      "step": 610
    },
    {
      "epoch": 0.5511111111111111,
      "grad_norm": 9.570143699645996,
      "learning_rate": 4.535726045439579e-05,
      "loss": 0.2802,
      "step": 620
    },
    {
      "epoch": 0.56,
      "grad_norm": 4.310057163238525,
      "learning_rate": 4.519262430029635e-05,
      "loss": 0.2495,
      "step": 630
    },
    {
      "epoch": 0.5688888888888889,
      "grad_norm": 12.770593643188477,
      "learning_rate": 4.502798814619691e-05,
      "loss": 0.271,
      "step": 640
    },
    {
      "epoch": 0.5777777777777777,
      "grad_norm": 6.601933002471924,
      "learning_rate": 4.486335199209747e-05,
      "loss": 0.3038,
      "step": 650
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 4.281738758087158,
      "learning_rate": 4.469871583799803e-05,
      "loss": 0.2832,
      "step": 660
    },
    {
      "epoch": 0.5955555555555555,
      "grad_norm": 4.757516860961914,
      "learning_rate": 4.453407968389859e-05,
      "loss": 0.2722,
      "step": 670
    },
    {
      "epoch": 0.6044444444444445,
      "grad_norm": 6.040764331817627,
      "learning_rate": 4.4369443529799146e-05,
      "loss": 0.2757,
      "step": 680
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 5.249866485595703,
      "learning_rate": 4.420480737569971e-05,
      "loss": 0.2592,
      "step": 690
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 4.697615146636963,
      "learning_rate": 4.404017122160027e-05,
      "loss": 0.2654,
      "step": 700
    },
    {
      "epoch": 0.6311111111111111,
      "grad_norm": 8.723139762878418,
      "learning_rate": 4.387553506750082e-05,
      "loss": 0.2505,
      "step": 710
    },
    {
      "epoch": 0.64,
      "grad_norm": 7.121882915496826,
      "learning_rate": 4.3710898913401383e-05,
      "loss": 0.2617,
      "step": 720
    },
    {
      "epoch": 0.6488888888888888,
      "grad_norm": 4.5506086349487305,
      "learning_rate": 4.354626275930194e-05,
      "loss": 0.2641,
      "step": 730
    },
    {
      "epoch": 0.6577777777777778,
      "grad_norm": 4.538687705993652,
      "learning_rate": 4.33816266052025e-05,
      "loss": 0.2541,
      "step": 740
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 4.556046009063721,
      "learning_rate": 4.3216990451103065e-05,
      "loss": 0.2574,
      "step": 750
    },
    {
      "epoch": 0.6755555555555556,
      "grad_norm": 5.514508247375488,
      "learning_rate": 4.305235429700362e-05,
      "loss": 0.2038,
      "step": 760
    },
    {
      "epoch": 0.6844444444444444,
      "grad_norm": 13.062164306640625,
      "learning_rate": 4.2887718142904184e-05,
      "loss": 0.2462,
      "step": 770
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 6.993749618530273,
      "learning_rate": 4.272308198880474e-05,
      "loss": 0.2881,
      "step": 780
    },
    {
      "epoch": 0.7022222222222222,
      "grad_norm": 6.145394325256348,
      "learning_rate": 4.25584458347053e-05,
      "loss": 0.2443,
      "step": 790
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 6.9502644538879395,
      "learning_rate": 4.239380968060586e-05,
      "loss": 0.2515,
      "step": 800
    },
    {
      "epoch": 0.72,
      "grad_norm": 5.197859764099121,
      "learning_rate": 4.222917352650642e-05,
      "loss": 0.2643,
      "step": 810
    },
    {
      "epoch": 0.7288888888888889,
      "grad_norm": 4.404785633087158,
      "learning_rate": 4.206453737240698e-05,
      "loss": 0.237,
      "step": 820
    },
    {
      "epoch": 0.7377777777777778,
      "grad_norm": 13.3486967086792,
      "learning_rate": 4.189990121830754e-05,
      "loss": 0.2516,
      "step": 830
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 4.863145351409912,
      "learning_rate": 4.1735265064208104e-05,
      "loss": 0.2673,
      "step": 840
    },
    {
      "epoch": 0.7555555555555555,
      "grad_norm": 3.8589117527008057,
      "learning_rate": 4.157062891010866e-05,
      "loss": 0.2522,
      "step": 850
    },
    {
      "epoch": 0.7644444444444445,
      "grad_norm": 7.2024922370910645,
      "learning_rate": 4.140599275600922e-05,
      "loss": 0.2165,
      "step": 860
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 3.396218776702881,
      "learning_rate": 4.124135660190978e-05,
      "loss": 0.2215,
      "step": 870
    },
    {
      "epoch": 0.7822222222222223,
      "grad_norm": 3.4670629501342773,
      "learning_rate": 4.107672044781034e-05,
      "loss": 0.2136,
      "step": 880
    },
    {
      "epoch": 0.7911111111111111,
      "grad_norm": 11.82852554321289,
      "learning_rate": 4.09120842937109e-05,
      "loss": 0.2412,
      "step": 890
    },
    {
      "epoch": 0.8,
      "grad_norm": 5.975481986999512,
      "learning_rate": 4.074744813961146e-05,
      "loss": 0.2409,
      "step": 900
    },
    {
      "epoch": 0.8088888888888889,
      "grad_norm": 6.422537803649902,
      "learning_rate": 4.058281198551202e-05,
      "loss": 0.2468,
      "step": 910
    },
    {
      "epoch": 0.8177777777777778,
      "grad_norm": 4.496952533721924,
      "learning_rate": 4.041817583141258e-05,
      "loss": 0.2266,
      "step": 920
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 8.867281913757324,
      "learning_rate": 4.025353967731314e-05,
      "loss": 0.2263,
      "step": 930
    },
    {
      "epoch": 0.8355555555555556,
      "grad_norm": 8.379363059997559,
      "learning_rate": 4.00889035232137e-05,
      "loss": 0.2006,
      "step": 940
    },
    {
      "epoch": 0.8444444444444444,
      "grad_norm": 7.734338760375977,
      "learning_rate": 3.992426736911426e-05,
      "loss": 0.2274,
      "step": 950
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 5.006381988525391,
      "learning_rate": 3.975963121501482e-05,
      "loss": 0.223,
      "step": 960
    },
    {
      "epoch": 0.8622222222222222,
      "grad_norm": 11.396899223327637,
      "learning_rate": 3.959499506091538e-05,
      "loss": 0.2105,
      "step": 970
    },
    {
      "epoch": 0.8711111111111111,
      "grad_norm": 6.2804365158081055,
      "learning_rate": 3.943035890681594e-05,
      "loss": 0.2247,
      "step": 980
    },
    {
      "epoch": 0.88,
      "grad_norm": 4.583906173706055,
      "learning_rate": 3.92657227527165e-05,
      "loss": 0.1891,
      "step": 990
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 13.17630672454834,
      "learning_rate": 3.910108659861706e-05,
      "loss": 0.2067,
      "step": 1000
    },
    {
      "epoch": 0.8977777777777778,
      "grad_norm": 4.534049034118652,
      "learning_rate": 3.893645044451762e-05,
      "loss": 0.1843,
      "step": 1010
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 6.500810146331787,
      "learning_rate": 3.877181429041818e-05,
      "loss": 0.1974,
      "step": 1020
    },
    {
      "epoch": 0.9155555555555556,
      "grad_norm": 5.061119079589844,
      "learning_rate": 3.8607178136318737e-05,
      "loss": 0.1978,
      "step": 1030
    },
    {
      "epoch": 0.9244444444444444,
      "grad_norm": 30.012229919433594,
      "learning_rate": 3.84425419822193e-05,
      "loss": 0.2238,
      "step": 1040
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 4.907500743865967,
      "learning_rate": 3.8277905828119855e-05,
      "loss": 0.2028,
      "step": 1050
    },
    {
      "epoch": 0.9422222222222222,
      "grad_norm": 4.947920799255371,
      "learning_rate": 3.811326967402042e-05,
      "loss": 0.2371,
      "step": 1060
    },
    {
      "epoch": 0.9511111111111111,
      "grad_norm": 4.512685298919678,
      "learning_rate": 3.794863351992098e-05,
      "loss": 0.199,
      "step": 1070
    },
    {
      "epoch": 0.96,
      "grad_norm": 8.559358596801758,
      "learning_rate": 3.778399736582154e-05,
      "loss": 0.2102,
      "step": 1080
    },
    {
      "epoch": 0.9688888888888889,
      "grad_norm": 4.5869598388671875,
      "learning_rate": 3.76193612117221e-05,
      "loss": 0.218,
      "step": 1090
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 3.3197083473205566,
      "learning_rate": 3.7454725057622656e-05,
      "loss": 0.2165,
      "step": 1100
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 5.053747653961182,
      "learning_rate": 3.729008890352322e-05,
      "loss": 0.2634,
      "step": 1110
    },
    {
      "epoch": 0.9955555555555555,
      "grad_norm": 4.858603000640869,
      "learning_rate": 3.7125452749423775e-05,
      "loss": 0.247,
      "step": 1120
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.9196,
      "eval_loss": 0.19855593144893646,
      "eval_runtime": 59.1393,
      "eval_samples_per_second": 169.092,
      "eval_steps_per_second": 8.455,
      "step": 1125
    },
    {
      "epoch": 1.0044444444444445,
      "grad_norm": 5.869924068450928,
      "learning_rate": 3.696081659532433e-05,
      "loss": 0.1719,
      "step": 1130
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 12.539472579956055,
      "learning_rate": 3.6796180441224894e-05,
      "loss": 0.1858,
      "step": 1140
    },
    {
      "epoch": 1.0222222222222221,
      "grad_norm": 3.85591197013855,
      "learning_rate": 3.663154428712545e-05,
      "loss": 0.1739,
      "step": 1150
    },
    {
      "epoch": 1.031111111111111,
      "grad_norm": 9.43132209777832,
      "learning_rate": 3.646690813302601e-05,
      "loss": 0.1465,
      "step": 1160
    },
    {
      "epoch": 1.04,
      "grad_norm": 5.766740322113037,
      "learning_rate": 3.630227197892657e-05,
      "loss": 0.1705,
      "step": 1170
    },
    {
      "epoch": 1.048888888888889,
      "grad_norm": 7.3928375244140625,
      "learning_rate": 3.613763582482713e-05,
      "loss": 0.1817,
      "step": 1180
    },
    {
      "epoch": 1.0577777777777777,
      "grad_norm": 5.286028861999512,
      "learning_rate": 3.5972999670727694e-05,
      "loss": 0.1746,
      "step": 1190
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 3.479860305786133,
      "learning_rate": 3.580836351662825e-05,
      "loss": 0.1896,
      "step": 1200
    },
    {
      "epoch": 1.0755555555555556,
      "grad_norm": 2.4450550079345703,
      "learning_rate": 3.564372736252881e-05,
      "loss": 0.1544,
      "step": 1210
    },
    {
      "epoch": 1.0844444444444445,
      "grad_norm": 8.941452980041504,
      "learning_rate": 3.547909120842937e-05,
      "loss": 0.17,
      "step": 1220
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 3.4936115741729736,
      "learning_rate": 3.531445505432993e-05,
      "loss": 0.1457,
      "step": 1230
    },
    {
      "epoch": 1.1022222222222222,
      "grad_norm": 7.342625141143799,
      "learning_rate": 3.514981890023049e-05,
      "loss": 0.1598,
      "step": 1240
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 7.3656511306762695,
      "learning_rate": 3.498518274613105e-05,
      "loss": 0.1589,
      "step": 1250
    },
    {
      "epoch": 1.12,
      "grad_norm": 3.9630260467529297,
      "learning_rate": 3.4820546592031614e-05,
      "loss": 0.1564,
      "step": 1260
    },
    {
      "epoch": 1.1288888888888888,
      "grad_norm": 6.677435874938965,
      "learning_rate": 3.465591043793217e-05,
      "loss": 0.1251,
      "step": 1270
    },
    {
      "epoch": 1.1377777777777778,
      "grad_norm": 5.903322696685791,
      "learning_rate": 3.449127428383273e-05,
      "loss": 0.1781,
      "step": 1280
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 6.786194801330566,
      "learning_rate": 3.432663812973329e-05,
      "loss": 0.1828,
      "step": 1290
    },
    {
      "epoch": 1.1555555555555554,
      "grad_norm": 14.04870891571045,
      "learning_rate": 3.416200197563385e-05,
      "loss": 0.1609,
      "step": 1300
    },
    {
      "epoch": 1.1644444444444444,
      "grad_norm": 12.208662986755371,
      "learning_rate": 3.399736582153441e-05,
      "loss": 0.1634,
      "step": 1310
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 6.3687028884887695,
      "learning_rate": 3.383272966743497e-05,
      "loss": 0.1613,
      "step": 1320
    },
    {
      "epoch": 1.1822222222222223,
      "grad_norm": 7.946609973907471,
      "learning_rate": 3.3668093513335534e-05,
      "loss": 0.1606,
      "step": 1330
    },
    {
      "epoch": 1.1911111111111112,
      "grad_norm": 20.45625877380371,
      "learning_rate": 3.350345735923609e-05,
      "loss": 0.142,
      "step": 1340
    },
    {
      "epoch": 1.2,
      "grad_norm": 3.072154998779297,
      "learning_rate": 3.333882120513665e-05,
      "loss": 0.1664,
      "step": 1350
    },
    {
      "epoch": 1.208888888888889,
      "grad_norm": 6.033754348754883,
      "learning_rate": 3.317418505103721e-05,
      "loss": 0.1367,
      "step": 1360
    },
    {
      "epoch": 1.2177777777777778,
      "grad_norm": 6.722637176513672,
      "learning_rate": 3.300954889693777e-05,
      "loss": 0.1831,
      "step": 1370
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 9.279434204101562,
      "learning_rate": 3.284491274283833e-05,
      "loss": 0.1902,
      "step": 1380
    },
    {
      "epoch": 1.2355555555555555,
      "grad_norm": 6.962395668029785,
      "learning_rate": 3.268027658873889e-05,
      "loss": 0.1618,
      "step": 1390
    },
    {
      "epoch": 1.2444444444444445,
      "grad_norm": 4.802653789520264,
      "learning_rate": 3.2515640434639446e-05,
      "loss": 0.1644,
      "step": 1400
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 9.518152236938477,
      "learning_rate": 3.235100428054001e-05,
      "loss": 0.1356,
      "step": 1410
    },
    {
      "epoch": 1.2622222222222224,
      "grad_norm": 5.804929733276367,
      "learning_rate": 3.218636812644057e-05,
      "loss": 0.1455,
      "step": 1420
    },
    {
      "epoch": 1.271111111111111,
      "grad_norm": 8.0812349319458,
      "learning_rate": 3.202173197234113e-05,
      "loss": 0.2024,
      "step": 1430
    },
    {
      "epoch": 1.28,
      "grad_norm": 7.584887504577637,
      "learning_rate": 3.185709581824169e-05,
      "loss": 0.158,
      "step": 1440
    },
    {
      "epoch": 1.2888888888888888,
      "grad_norm": 5.456803321838379,
      "learning_rate": 3.169245966414225e-05,
      "loss": 0.1304,
      "step": 1450
    },
    {
      "epoch": 1.2977777777777777,
      "grad_norm": 7.687901973724365,
      "learning_rate": 3.152782351004281e-05,
      "loss": 0.1509,
      "step": 1460
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 5.4591264724731445,
      "learning_rate": 3.1363187355943366e-05,
      "loss": 0.1686,
      "step": 1470
    },
    {
      "epoch": 1.3155555555555556,
      "grad_norm": 3.245868682861328,
      "learning_rate": 3.119855120184393e-05,
      "loss": 0.1444,
      "step": 1480
    },
    {
      "epoch": 1.3244444444444445,
      "grad_norm": 4.5031585693359375,
      "learning_rate": 3.103391504774449e-05,
      "loss": 0.1261,
      "step": 1490
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 3.2949371337890625,
      "learning_rate": 3.086927889364505e-05,
      "loss": 0.1752,
      "step": 1500
    },
    {
      "epoch": 1.3422222222222222,
      "grad_norm": 3.51379656791687,
      "learning_rate": 3.070464273954561e-05,
      "loss": 0.133,
      "step": 1510
    },
    {
      "epoch": 1.3511111111111112,
      "grad_norm": 4.348755359649658,
      "learning_rate": 3.0540006585446166e-05,
      "loss": 0.1437,
      "step": 1520
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 11.595159530639648,
      "learning_rate": 3.0375370431346723e-05,
      "loss": 0.152,
      "step": 1530
    },
    {
      "epoch": 1.3688888888888888,
      "grad_norm": 3.5267624855041504,
      "learning_rate": 3.0210734277247282e-05,
      "loss": 0.1837,
      "step": 1540
    },
    {
      "epoch": 1.3777777777777778,
      "grad_norm": 7.104850769042969,
      "learning_rate": 3.004609812314784e-05,
      "loss": 0.1691,
      "step": 1550
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 5.069340705871582,
      "learning_rate": 2.98814619690484e-05,
      "loss": 0.1727,
      "step": 1560
    },
    {
      "epoch": 1.3955555555555557,
      "grad_norm": 6.879201889038086,
      "learning_rate": 2.9716825814948964e-05,
      "loss": 0.1522,
      "step": 1570
    },
    {
      "epoch": 1.4044444444444444,
      "grad_norm": 6.443312168121338,
      "learning_rate": 2.9552189660849523e-05,
      "loss": 0.1519,
      "step": 1580
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 8.480443000793457,
      "learning_rate": 2.9387553506750083e-05,
      "loss": 0.1299,
      "step": 1590
    },
    {
      "epoch": 1.4222222222222223,
      "grad_norm": 5.005562782287598,
      "learning_rate": 2.9222917352650642e-05,
      "loss": 0.1516,
      "step": 1600
    },
    {
      "epoch": 1.431111111111111,
      "grad_norm": 7.9113030433654785,
      "learning_rate": 2.90582811985512e-05,
      "loss": 0.1572,
      "step": 1610
    },
    {
      "epoch": 1.44,
      "grad_norm": 8.467059135437012,
      "learning_rate": 2.889364504445176e-05,
      "loss": 0.139,
      "step": 1620
    },
    {
      "epoch": 1.448888888888889,
      "grad_norm": 7.292123794555664,
      "learning_rate": 2.872900889035232e-05,
      "loss": 0.1469,
      "step": 1630
    },
    {
      "epoch": 1.4577777777777778,
      "grad_norm": 11.055058479309082,
      "learning_rate": 2.856437273625288e-05,
      "loss": 0.1669,
      "step": 1640
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 3.2929635047912598,
      "learning_rate": 2.8399736582153443e-05,
      "loss": 0.1553,
      "step": 1650
    },
    {
      "epoch": 1.4755555555555555,
      "grad_norm": 7.299927711486816,
      "learning_rate": 2.8235100428054002e-05,
      "loss": 0.1302,
      "step": 1660
    },
    {
      "epoch": 1.4844444444444445,
      "grad_norm": 4.5059051513671875,
      "learning_rate": 2.807046427395456e-05,
      "loss": 0.1267,
      "step": 1670
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 16.13638687133789,
      "learning_rate": 2.790582811985512e-05,
      "loss": 0.1989,
      "step": 1680
    },
    {
      "epoch": 1.5022222222222221,
      "grad_norm": 4.637728691101074,
      "learning_rate": 2.774119196575568e-05,
      "loss": 0.1575,
      "step": 1690
    },
    {
      "epoch": 1.511111111111111,
      "grad_norm": 9.142492294311523,
      "learning_rate": 2.757655581165624e-05,
      "loss": 0.1544,
      "step": 1700
    },
    {
      "epoch": 1.52,
      "grad_norm": 2.0825304985046387,
      "learning_rate": 2.74119196575568e-05,
      "loss": 0.1251,
      "step": 1710
    },
    {
      "epoch": 1.528888888888889,
      "grad_norm": 8.693547248840332,
      "learning_rate": 2.724728350345736e-05,
      "loss": 0.1751,
      "step": 1720
    },
    {
      "epoch": 1.537777777777778,
      "grad_norm": 5.092342853546143,
      "learning_rate": 2.708264734935792e-05,
      "loss": 0.1589,
      "step": 1730
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 3.7054131031036377,
      "learning_rate": 2.691801119525848e-05,
      "loss": 0.1511,
      "step": 1740
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 2.2196061611175537,
      "learning_rate": 2.675337504115904e-05,
      "loss": 0.1317,
      "step": 1750
    },
    {
      "epoch": 1.5644444444444443,
      "grad_norm": 4.126953125,
      "learning_rate": 2.65887388870596e-05,
      "loss": 0.1402,
      "step": 1760
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 4.30802583694458,
      "learning_rate": 2.642410273296016e-05,
      "loss": 0.1012,
      "step": 1770
    },
    {
      "epoch": 1.5822222222222222,
      "grad_norm": 6.516756534576416,
      "learning_rate": 2.625946657886072e-05,
      "loss": 0.1281,
      "step": 1780
    },
    {
      "epoch": 1.5911111111111111,
      "grad_norm": 9.875994682312012,
      "learning_rate": 2.609483042476128e-05,
      "loss": 0.1512,
      "step": 1790
    },
    {
      "epoch": 1.6,
      "grad_norm": 4.856042861938477,
      "learning_rate": 2.593019427066184e-05,
      "loss": 0.1212,
      "step": 1800
    },
    {
      "epoch": 1.608888888888889,
      "grad_norm": 7.17677116394043,
      "learning_rate": 2.57655581165624e-05,
      "loss": 0.1476,
      "step": 1810
    },
    {
      "epoch": 1.6177777777777778,
      "grad_norm": 3.873084306716919,
      "learning_rate": 2.560092196246296e-05,
      "loss": 0.1347,
      "step": 1820
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 5.028093338012695,
      "learning_rate": 2.543628580836352e-05,
      "loss": 0.1601,
      "step": 1830
    },
    {
      "epoch": 1.6355555555555554,
      "grad_norm": 6.34876823425293,
      "learning_rate": 2.527164965426408e-05,
      "loss": 0.1359,
      "step": 1840
    },
    {
      "epoch": 1.6444444444444444,
      "grad_norm": 5.476853847503662,
      "learning_rate": 2.510701350016464e-05,
      "loss": 0.1294,
      "step": 1850
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 9.206507682800293,
      "learning_rate": 2.4942377346065194e-05,
      "loss": 0.138,
      "step": 1860
    },
    {
      "epoch": 1.6622222222222223,
      "grad_norm": 3.6985976696014404,
      "learning_rate": 2.4777741191965757e-05,
      "loss": 0.1293,
      "step": 1870
    },
    {
      "epoch": 1.6711111111111112,
      "grad_norm": 2.4266791343688965,
      "learning_rate": 2.4613105037866317e-05,
      "loss": 0.1189,
      "step": 1880
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 14.982433319091797,
      "learning_rate": 2.4448468883766876e-05,
      "loss": 0.1587,
      "step": 1890
    },
    {
      "epoch": 1.6888888888888889,
      "grad_norm": 2.949382781982422,
      "learning_rate": 2.4283832729667436e-05,
      "loss": 0.1279,
      "step": 1900
    },
    {
      "epoch": 1.6977777777777778,
      "grad_norm": 67.59186553955078,
      "learning_rate": 2.4119196575567995e-05,
      "loss": 0.1339,
      "step": 1910
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 8.264497756958008,
      "learning_rate": 2.3954560421468555e-05,
      "loss": 0.1394,
      "step": 1920
    },
    {
      "epoch": 1.7155555555555555,
      "grad_norm": 7.046630382537842,
      "learning_rate": 2.3789924267369114e-05,
      "loss": 0.1256,
      "step": 1930
    },
    {
      "epoch": 1.7244444444444444,
      "grad_norm": 5.093416690826416,
      "learning_rate": 2.3625288113269677e-05,
      "loss": 0.1404,
      "step": 1940
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 4.658504962921143,
      "learning_rate": 2.3460651959170236e-05,
      "loss": 0.1456,
      "step": 1950
    },
    {
      "epoch": 1.7422222222222223,
      "grad_norm": 6.8234710693359375,
      "learning_rate": 2.3296015805070796e-05,
      "loss": 0.1337,
      "step": 1960
    },
    {
      "epoch": 1.751111111111111,
      "grad_norm": 4.598105430603027,
      "learning_rate": 2.3131379650971355e-05,
      "loss": 0.1381,
      "step": 1970
    },
    {
      "epoch": 1.76,
      "grad_norm": 9.007284164428711,
      "learning_rate": 2.2966743496871915e-05,
      "loss": 0.1237,
      "step": 1980
    },
    {
      "epoch": 1.7688888888888887,
      "grad_norm": 5.078076362609863,
      "learning_rate": 2.2802107342772474e-05,
      "loss": 0.1188,
      "step": 1990
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 4.986614227294922,
      "learning_rate": 2.2637471188673034e-05,
      "loss": 0.1269,
      "step": 2000
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 3.720546245574951,
      "learning_rate": 2.2472835034573593e-05,
      "loss": 0.1132,
      "step": 2010
    },
    {
      "epoch": 1.7955555555555556,
      "grad_norm": 8.251795768737793,
      "learning_rate": 2.2308198880474156e-05,
      "loss": 0.1478,
      "step": 2020
    },
    {
      "epoch": 1.8044444444444445,
      "grad_norm": 3.69931697845459,
      "learning_rate": 2.2143562726374715e-05,
      "loss": 0.1266,
      "step": 2030
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 9.563814163208008,
      "learning_rate": 2.1978926572275275e-05,
      "loss": 0.144,
      "step": 2040
    },
    {
      "epoch": 1.8222222222222222,
      "grad_norm": 7.24586296081543,
      "learning_rate": 2.181429041817583e-05,
      "loss": 0.1196,
      "step": 2050
    },
    {
      "epoch": 1.8311111111111111,
      "grad_norm": 7.24547004699707,
      "learning_rate": 2.164965426407639e-05,
      "loss": 0.1402,
      "step": 2060
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 8.910736083984375,
      "learning_rate": 2.148501810997695e-05,
      "loss": 0.1152,
      "step": 2070
    },
    {
      "epoch": 1.8488888888888888,
      "grad_norm": 5.82919454574585,
      "learning_rate": 2.1320381955877512e-05,
      "loss": 0.1125,
      "step": 2080
    },
    {
      "epoch": 1.8577777777777778,
      "grad_norm": 27.97666358947754,
      "learning_rate": 2.1155745801778072e-05,
      "loss": 0.1606,
      "step": 2090
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 7.618607997894287,
      "learning_rate": 2.099110964767863e-05,
      "loss": 0.133,
      "step": 2100
    },
    {
      "epoch": 1.8755555555555556,
      "grad_norm": 3.616060733795166,
      "learning_rate": 2.082647349357919e-05,
      "loss": 0.1455,
      "step": 2110
    },
    {
      "epoch": 1.8844444444444446,
      "grad_norm": 6.923144817352295,
      "learning_rate": 2.066183733947975e-05,
      "loss": 0.1285,
      "step": 2120
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 4.769718647003174,
      "learning_rate": 2.049720118538031e-05,
      "loss": 0.1258,
      "step": 2130
    },
    {
      "epoch": 1.9022222222222223,
      "grad_norm": 4.009350299835205,
      "learning_rate": 2.033256503128087e-05,
      "loss": 0.1169,
      "step": 2140
    },
    {
      "epoch": 1.911111111111111,
      "grad_norm": 17.89965057373047,
      "learning_rate": 2.016792887718143e-05,
      "loss": 0.1122,
      "step": 2150
    },
    {
      "epoch": 1.92,
      "grad_norm": 4.5547661781311035,
      "learning_rate": 2.000329272308199e-05,
      "loss": 0.1462,
      "step": 2160
    },
    {
      "epoch": 1.9288888888888889,
      "grad_norm": 4.184633731842041,
      "learning_rate": 1.983865656898255e-05,
      "loss": 0.1397,
      "step": 2170
    },
    {
      "epoch": 1.9377777777777778,
      "grad_norm": 5.103354454040527,
      "learning_rate": 1.967402041488311e-05,
      "loss": 0.1236,
      "step": 2180
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 7.15592098236084,
      "learning_rate": 1.950938426078367e-05,
      "loss": 0.1501,
      "step": 2190
    },
    {
      "epoch": 1.9555555555555557,
      "grad_norm": 4.1310529708862305,
      "learning_rate": 1.934474810668423e-05,
      "loss": 0.131,
      "step": 2200
    },
    {
      "epoch": 1.9644444444444444,
      "grad_norm": 10.8958740234375,
      "learning_rate": 1.918011195258479e-05,
      "loss": 0.1527,
      "step": 2210
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 5.677941799163818,
      "learning_rate": 1.9015475798485348e-05,
      "loss": 0.1268,
      "step": 2220
    },
    {
      "epoch": 1.982222222222222,
      "grad_norm": 5.539301872253418,
      "learning_rate": 1.8850839644385908e-05,
      "loss": 0.1167,
      "step": 2230
    },
    {
      "epoch": 1.991111111111111,
      "grad_norm": 8.705559730529785,
      "learning_rate": 1.868620349028647e-05,
      "loss": 0.1236,
      "step": 2240
    },
    {
      "epoch": 2.0,
      "grad_norm": 12.138946533203125,
      "learning_rate": 1.8521567336187026e-05,
      "loss": 0.1294,
      "step": 2250
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.9419,
      "eval_loss": 0.1527385711669922,
      "eval_runtime": 59.1343,
      "eval_samples_per_second": 169.107,
      "eval_steps_per_second": 8.455,
      "step": 2250
    },
    {
      "epoch": 2.008888888888889,
      "grad_norm": 6.825613975524902,
      "learning_rate": 1.8356931182087586e-05,
      "loss": 0.0682,
      "step": 2260
    },
    {
      "epoch": 2.017777777777778,
      "grad_norm": 7.322240352630615,
      "learning_rate": 1.8192295027988145e-05,
      "loss": 0.0849,
      "step": 2270
    },
    {
      "epoch": 2.026666666666667,
      "grad_norm": 7.021109580993652,
      "learning_rate": 1.8027658873888705e-05,
      "loss": 0.0722,
      "step": 2280
    },
    {
      "epoch": 2.0355555555555553,
      "grad_norm": 6.651477336883545,
      "learning_rate": 1.7863022719789264e-05,
      "loss": 0.0633,
      "step": 2290
    },
    {
      "epoch": 2.0444444444444443,
      "grad_norm": 5.285604953765869,
      "learning_rate": 1.7698386565689827e-05,
      "loss": 0.0784,
      "step": 2300
    },
    {
      "epoch": 2.0533333333333332,
      "grad_norm": 2.429126739501953,
      "learning_rate": 1.7533750411590387e-05,
      "loss": 0.0535,
      "step": 2310
    },
    {
      "epoch": 2.062222222222222,
      "grad_norm": 5.946181297302246,
      "learning_rate": 1.7369114257490946e-05,
      "loss": 0.0955,
      "step": 2320
    },
    {
      "epoch": 2.071111111111111,
      "grad_norm": 2.8704214096069336,
      "learning_rate": 1.7204478103391505e-05,
      "loss": 0.0815,
      "step": 2330
    },
    {
      "epoch": 2.08,
      "grad_norm": 7.774537086486816,
      "learning_rate": 1.7039841949292065e-05,
      "loss": 0.0773,
      "step": 2340
    },
    {
      "epoch": 2.088888888888889,
      "grad_norm": 4.649056434631348,
      "learning_rate": 1.6875205795192624e-05,
      "loss": 0.0774,
      "step": 2350
    },
    {
      "epoch": 2.097777777777778,
      "grad_norm": 8.55126667022705,
      "learning_rate": 1.6710569641093184e-05,
      "loss": 0.0869,
      "step": 2360
    },
    {
      "epoch": 2.1066666666666665,
      "grad_norm": 8.691363334655762,
      "learning_rate": 1.6545933486993743e-05,
      "loss": 0.0891,
      "step": 2370
    },
    {
      "epoch": 2.1155555555555554,
      "grad_norm": 7.648005962371826,
      "learning_rate": 1.6381297332894306e-05,
      "loss": 0.111,
      "step": 2380
    },
    {
      "epoch": 2.1244444444444444,
      "grad_norm": 12.933874130249023,
      "learning_rate": 1.6216661178794866e-05,
      "loss": 0.0907,
      "step": 2390
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 7.021823883056641,
      "learning_rate": 1.6052025024695425e-05,
      "loss": 0.0923,
      "step": 2400
    },
    {
      "epoch": 2.1422222222222222,
      "grad_norm": 6.998866081237793,
      "learning_rate": 1.5887388870595984e-05,
      "loss": 0.0799,
      "step": 2410
    },
    {
      "epoch": 2.151111111111111,
      "grad_norm": 14.014531135559082,
      "learning_rate": 1.5722752716496544e-05,
      "loss": 0.0898,
      "step": 2420
    },
    {
      "epoch": 2.16,
      "grad_norm": 4.758207321166992,
      "learning_rate": 1.5558116562397103e-05,
      "loss": 0.0625,
      "step": 2430
    },
    {
      "epoch": 2.168888888888889,
      "grad_norm": 5.972497940063477,
      "learning_rate": 1.5393480408297663e-05,
      "loss": 0.0948,
      "step": 2440
    },
    {
      "epoch": 2.1777777777777776,
      "grad_norm": 2.27144455909729,
      "learning_rate": 1.5228844254198224e-05,
      "loss": 0.0715,
      "step": 2450
    },
    {
      "epoch": 2.1866666666666665,
      "grad_norm": 8.910465240478516,
      "learning_rate": 1.5064208100098782e-05,
      "loss": 0.0763,
      "step": 2460
    },
    {
      "epoch": 2.1955555555555555,
      "grad_norm": 5.998537063598633,
      "learning_rate": 1.4899571945999341e-05,
      "loss": 0.0785,
      "step": 2470
    },
    {
      "epoch": 2.2044444444444444,
      "grad_norm": 0.7041501402854919,
      "learning_rate": 1.47349357918999e-05,
      "loss": 0.0697,
      "step": 2480
    },
    {
      "epoch": 2.2133333333333334,
      "grad_norm": 3.7292842864990234,
      "learning_rate": 1.457029963780046e-05,
      "loss": 0.0828,
      "step": 2490
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 5.514894962310791,
      "learning_rate": 1.4405663483701021e-05,
      "loss": 0.0793,
      "step": 2500
    },
    {
      "epoch": 2.2311111111111113,
      "grad_norm": 9.477383613586426,
      "learning_rate": 1.424102732960158e-05,
      "loss": 0.0546,
      "step": 2510
    },
    {
      "epoch": 2.24,
      "grad_norm": 3.6585450172424316,
      "learning_rate": 1.407639117550214e-05,
      "loss": 0.0695,
      "step": 2520
    },
    {
      "epoch": 2.2488888888888887,
      "grad_norm": 2.4826953411102295,
      "learning_rate": 1.3911755021402701e-05,
      "loss": 0.0561,
      "step": 2530
    },
    {
      "epoch": 2.2577777777777777,
      "grad_norm": 1.4108823537826538,
      "learning_rate": 1.374711886730326e-05,
      "loss": 0.0654,
      "step": 2540
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 2.7887392044067383,
      "learning_rate": 1.358248271320382e-05,
      "loss": 0.0496,
      "step": 2550
    },
    {
      "epoch": 2.2755555555555556,
      "grad_norm": 2.9599502086639404,
      "learning_rate": 1.341784655910438e-05,
      "loss": 0.0652,
      "step": 2560
    },
    {
      "epoch": 2.2844444444444445,
      "grad_norm": 7.221150875091553,
      "learning_rate": 1.325321040500494e-05,
      "loss": 0.1078,
      "step": 2570
    },
    {
      "epoch": 2.2933333333333334,
      "grad_norm": 5.851492881774902,
      "learning_rate": 1.30885742509055e-05,
      "loss": 0.0748,
      "step": 2580
    },
    {
      "epoch": 2.3022222222222224,
      "grad_norm": 3.440951108932495,
      "learning_rate": 1.292393809680606e-05,
      "loss": 0.0505,
      "step": 2590
    },
    {
      "epoch": 2.311111111111111,
      "grad_norm": 4.185608863830566,
      "learning_rate": 1.2759301942706619e-05,
      "loss": 0.0616,
      "step": 2600
    },
    {
      "epoch": 2.32,
      "grad_norm": 7.528196334838867,
      "learning_rate": 1.259466578860718e-05,
      "loss": 0.0712,
      "step": 2610
    },
    {
      "epoch": 2.328888888888889,
      "grad_norm": 4.733784198760986,
      "learning_rate": 1.2430029634507738e-05,
      "loss": 0.0747,
      "step": 2620
    },
    {
      "epoch": 2.3377777777777777,
      "grad_norm": 10.200396537780762,
      "learning_rate": 1.2265393480408297e-05,
      "loss": 0.088,
      "step": 2630
    },
    {
      "epoch": 2.3466666666666667,
      "grad_norm": 4.077938079833984,
      "learning_rate": 1.2100757326308859e-05,
      "loss": 0.0522,
      "step": 2640
    },
    {
      "epoch": 2.3555555555555556,
      "grad_norm": 7.3976945877075195,
      "learning_rate": 1.1936121172209418e-05,
      "loss": 0.075,
      "step": 2650
    },
    {
      "epoch": 2.3644444444444446,
      "grad_norm": 3.3763163089752197,
      "learning_rate": 1.1771485018109977e-05,
      "loss": 0.0811,
      "step": 2660
    },
    {
      "epoch": 2.3733333333333335,
      "grad_norm": 3.232412099838257,
      "learning_rate": 1.1606848864010537e-05,
      "loss": 0.0691,
      "step": 2670
    },
    {
      "epoch": 2.3822222222222225,
      "grad_norm": 4.869272232055664,
      "learning_rate": 1.1442212709911098e-05,
      "loss": 0.0733,
      "step": 2680
    },
    {
      "epoch": 2.391111111111111,
      "grad_norm": 4.703092575073242,
      "learning_rate": 1.1277576555811657e-05,
      "loss": 0.0697,
      "step": 2690
    },
    {
      "epoch": 2.4,
      "grad_norm": 5.713374137878418,
      "learning_rate": 1.1112940401712217e-05,
      "loss": 0.0759,
      "step": 2700
    },
    {
      "epoch": 2.408888888888889,
      "grad_norm": 5.307226657867432,
      "learning_rate": 1.0948304247612776e-05,
      "loss": 0.0506,
      "step": 2710
    },
    {
      "epoch": 2.417777777777778,
      "grad_norm": 16.72958755493164,
      "learning_rate": 1.0783668093513336e-05,
      "loss": 0.0666,
      "step": 2720
    },
    {
      "epoch": 2.4266666666666667,
      "grad_norm": 4.859306812286377,
      "learning_rate": 1.0619031939413895e-05,
      "loss": 0.0488,
      "step": 2730
    },
    {
      "epoch": 2.4355555555555557,
      "grad_norm": 6.999468803405762,
      "learning_rate": 1.0454395785314455e-05,
      "loss": 0.0963,
      "step": 2740
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 10.78720760345459,
      "learning_rate": 1.0289759631215016e-05,
      "loss": 0.0783,
      "step": 2750
    },
    {
      "epoch": 2.453333333333333,
      "grad_norm": 8.634557723999023,
      "learning_rate": 1.0125123477115575e-05,
      "loss": 0.0757,
      "step": 2760
    },
    {
      "epoch": 2.462222222222222,
      "grad_norm": 5.156482696533203,
      "learning_rate": 9.960487323016135e-06,
      "loss": 0.0459,
      "step": 2770
    },
    {
      "epoch": 2.471111111111111,
      "grad_norm": 12.313836097717285,
      "learning_rate": 9.795851168916694e-06,
      "loss": 0.0847,
      "step": 2780
    },
    {
      "epoch": 2.48,
      "grad_norm": 3.8935000896453857,
      "learning_rate": 9.631215014817255e-06,
      "loss": 0.0664,
      "step": 2790
    },
    {
      "epoch": 2.488888888888889,
      "grad_norm": 8.88865852355957,
      "learning_rate": 9.466578860717815e-06,
      "loss": 0.0856,
      "step": 2800
    },
    {
      "epoch": 2.497777777777778,
      "grad_norm": 3.2222251892089844,
      "learning_rate": 9.301942706618374e-06,
      "loss": 0.0677,
      "step": 2810
    },
    {
      "epoch": 2.506666666666667,
      "grad_norm": 8.055662155151367,
      "learning_rate": 9.137306552518934e-06,
      "loss": 0.0829,
      "step": 2820
    },
    {
      "epoch": 2.5155555555555553,
      "grad_norm": 5.750452995300293,
      "learning_rate": 8.972670398419493e-06,
      "loss": 0.0568,
      "step": 2830
    },
    {
      "epoch": 2.5244444444444447,
      "grad_norm": 3.874258518218994,
      "learning_rate": 8.808034244320053e-06,
      "loss": 0.0506,
      "step": 2840
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 15.960229873657227,
      "learning_rate": 8.643398090220612e-06,
      "loss": 0.0849,
      "step": 2850
    },
    {
      "epoch": 2.542222222222222,
      "grad_norm": 3.417994737625122,
      "learning_rate": 8.478761936121173e-06,
      "loss": 0.0839,
      "step": 2860
    },
    {
      "epoch": 2.551111111111111,
      "grad_norm": 4.8730645179748535,
      "learning_rate": 8.314125782021733e-06,
      "loss": 0.0591,
      "step": 2870
    },
    {
      "epoch": 2.56,
      "grad_norm": 4.505559921264648,
      "learning_rate": 8.149489627922292e-06,
      "loss": 0.078,
      "step": 2880
    },
    {
      "epoch": 2.568888888888889,
      "grad_norm": 14.278441429138184,
      "learning_rate": 7.984853473822852e-06,
      "loss": 0.0569,
      "step": 2890
    },
    {
      "epoch": 2.5777777777777775,
      "grad_norm": 6.640810966491699,
      "learning_rate": 7.820217319723413e-06,
      "loss": 0.0718,
      "step": 2900
    },
    {
      "epoch": 2.586666666666667,
      "grad_norm": 5.148899078369141,
      "learning_rate": 7.655581165623972e-06,
      "loss": 0.0794,
      "step": 2910
    },
    {
      "epoch": 2.5955555555555554,
      "grad_norm": 5.5339813232421875,
      "learning_rate": 7.490945011524531e-06,
      "loss": 0.062,
      "step": 2920
    },
    {
      "epoch": 2.6044444444444443,
      "grad_norm": 9.063589096069336,
      "learning_rate": 7.32630885742509e-06,
      "loss": 0.052,
      "step": 2930
    },
    {
      "epoch": 2.6133333333333333,
      "grad_norm": 5.45584774017334,
      "learning_rate": 7.1616727033256504e-06,
      "loss": 0.0666,
      "step": 2940
    },
    {
      "epoch": 2.6222222222222222,
      "grad_norm": 7.607778549194336,
      "learning_rate": 6.99703654922621e-06,
      "loss": 0.0732,
      "step": 2950
    },
    {
      "epoch": 2.631111111111111,
      "grad_norm": 0.8555558919906616,
      "learning_rate": 6.83240039512677e-06,
      "loss": 0.0334,
      "step": 2960
    },
    {
      "epoch": 2.64,
      "grad_norm": 12.65419864654541,
      "learning_rate": 6.66776424102733e-06,
      "loss": 0.0674,
      "step": 2970
    },
    {
      "epoch": 2.648888888888889,
      "grad_norm": 5.392725467681885,
      "learning_rate": 6.50312808692789e-06,
      "loss": 0.0796,
      "step": 2980
    },
    {
      "epoch": 2.6577777777777776,
      "grad_norm": 4.71859884262085,
      "learning_rate": 6.338491932828449e-06,
      "loss": 0.0769,
      "step": 2990
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 8.127032279968262,
      "learning_rate": 6.173855778729009e-06,
      "loss": 0.0734,
      "step": 3000
    },
    {
      "epoch": 2.6755555555555555,
      "grad_norm": 8.99980354309082,
      "learning_rate": 6.009219624629569e-06,
      "loss": 0.0639,
      "step": 3010
    },
    {
      "epoch": 2.6844444444444444,
      "grad_norm": 2.834132194519043,
      "learning_rate": 5.8445834705301286e-06,
      "loss": 0.0598,
      "step": 3020
    },
    {
      "epoch": 2.6933333333333334,
      "grad_norm": 8.139854431152344,
      "learning_rate": 5.679947316430689e-06,
      "loss": 0.0911,
      "step": 3030
    },
    {
      "epoch": 2.7022222222222223,
      "grad_norm": 4.081265926361084,
      "learning_rate": 5.515311162331248e-06,
      "loss": 0.0648,
      "step": 3040
    },
    {
      "epoch": 2.7111111111111112,
      "grad_norm": 4.275919437408447,
      "learning_rate": 5.350675008231808e-06,
      "loss": 0.0513,
      "step": 3050
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 6.741747856140137,
      "learning_rate": 5.186038854132367e-06,
      "loss": 0.034,
      "step": 3060
    },
    {
      "epoch": 2.728888888888889,
      "grad_norm": 19.525732040405273,
      "learning_rate": 5.0214027000329275e-06,
      "loss": 0.0637,
      "step": 3070
    },
    {
      "epoch": 2.7377777777777776,
      "grad_norm": 2.379011392593384,
      "learning_rate": 4.856766545933488e-06,
      "loss": 0.0544,
      "step": 3080
    },
    {
      "epoch": 2.7466666666666666,
      "grad_norm": 3.20642352104187,
      "learning_rate": 4.692130391834047e-06,
      "loss": 0.0698,
      "step": 3090
    },
    {
      "epoch": 2.7555555555555555,
      "grad_norm": 4.52004861831665,
      "learning_rate": 4.527494237734607e-06,
      "loss": 0.0488,
      "step": 3100
    },
    {
      "epoch": 2.7644444444444445,
      "grad_norm": 10.666729927062988,
      "learning_rate": 4.362858083635166e-06,
      "loss": 0.0824,
      "step": 3110
    },
    {
      "epoch": 2.7733333333333334,
      "grad_norm": 11.918787956237793,
      "learning_rate": 4.1982219295357264e-06,
      "loss": 0.084,
      "step": 3120
    },
    {
      "epoch": 2.7822222222222224,
      "grad_norm": 2.3286819458007812,
      "learning_rate": 4.033585775436286e-06,
      "loss": 0.0614,
      "step": 3130
    },
    {
      "epoch": 2.7911111111111113,
      "grad_norm": 14.16545295715332,
      "learning_rate": 3.868949621336846e-06,
      "loss": 0.0806,
      "step": 3140
    },
    {
      "epoch": 2.8,
      "grad_norm": 3.8670270442962646,
      "learning_rate": 3.7043134672374052e-06,
      "loss": 0.0561,
      "step": 3150
    },
    {
      "epoch": 2.8088888888888888,
      "grad_norm": 11.627376556396484,
      "learning_rate": 3.539677313137965e-06,
      "loss": 0.0898,
      "step": 3160
    },
    {
      "epoch": 2.8177777777777777,
      "grad_norm": 6.504073619842529,
      "learning_rate": 3.375041159038525e-06,
      "loss": 0.0749,
      "step": 3170
    },
    {
      "epoch": 2.8266666666666667,
      "grad_norm": 9.000718116760254,
      "learning_rate": 3.210405004939085e-06,
      "loss": 0.0687,
      "step": 3180
    },
    {
      "epoch": 2.8355555555555556,
      "grad_norm": 5.028514385223389,
      "learning_rate": 3.0457688508396443e-06,
      "loss": 0.0448,
      "step": 3190
    },
    {
      "epoch": 2.8444444444444446,
      "grad_norm": 2.587144374847412,
      "learning_rate": 2.881132696740204e-06,
      "loss": 0.0435,
      "step": 3200
    },
    {
      "epoch": 2.8533333333333335,
      "grad_norm": 2.2906277179718018,
      "learning_rate": 2.716496542640764e-06,
      "loss": 0.0483,
      "step": 3210
    },
    {
      "epoch": 2.862222222222222,
      "grad_norm": 2.1080408096313477,
      "learning_rate": 2.551860388541324e-06,
      "loss": 0.0555,
      "step": 3220
    },
    {
      "epoch": 2.871111111111111,
      "grad_norm": 8.761284828186035,
      "learning_rate": 2.3872242344418838e-06,
      "loss": 0.0753,
      "step": 3230
    },
    {
      "epoch": 2.88,
      "grad_norm": 6.168934345245361,
      "learning_rate": 2.2225880803424432e-06,
      "loss": 0.0482,
      "step": 3240
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 1.588948130607605,
      "learning_rate": 2.057951926243003e-06,
      "loss": 0.0659,
      "step": 3250
    },
    {
      "epoch": 2.897777777777778,
      "grad_norm": 3.5567994117736816,
      "learning_rate": 1.8933157721435627e-06,
      "loss": 0.0571,
      "step": 3260
    },
    {
      "epoch": 2.9066666666666667,
      "grad_norm": 3.8851680755615234,
      "learning_rate": 1.7286796180441226e-06,
      "loss": 0.0585,
      "step": 3270
    },
    {
      "epoch": 2.9155555555555557,
      "grad_norm": 4.359981536865234,
      "learning_rate": 1.5640434639446825e-06,
      "loss": 0.0609,
      "step": 3280
    },
    {
      "epoch": 2.924444444444444,
      "grad_norm": 3.856964588165283,
      "learning_rate": 1.3994073098452421e-06,
      "loss": 0.0346,
      "step": 3290
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 19.95442008972168,
      "learning_rate": 1.2347711557458018e-06,
      "loss": 0.0582,
      "step": 3300
    },
    {
      "epoch": 2.942222222222222,
      "grad_norm": 5.607608795166016,
      "learning_rate": 1.0701350016463617e-06,
      "loss": 0.0385,
      "step": 3310
    },
    {
      "epoch": 2.951111111111111,
      "grad_norm": 1.8074361085891724,
      "learning_rate": 9.054988475469213e-07,
      "loss": 0.0567,
      "step": 3320
    },
    {
      "epoch": 2.96,
      "grad_norm": 3.162261486053467,
      "learning_rate": 7.408626934474811e-07,
      "loss": 0.065,
      "step": 3330
    },
    {
      "epoch": 2.968888888888889,
      "grad_norm": 3.8333547115325928,
      "learning_rate": 5.762265393480408e-07,
      "loss": 0.0637,
      "step": 3340
    },
    {
      "epoch": 2.977777777777778,
      "grad_norm": 4.963481903076172,
      "learning_rate": 4.1159038524860064e-07,
      "loss": 0.0631,
      "step": 3350
    },
    {
      "epoch": 2.986666666666667,
      "grad_norm": 3.250166416168213,
      "learning_rate": 2.469542311491604e-07,
      "loss": 0.0565,
      "step": 3360
    },
    {
      "epoch": 2.9955555555555557,
      "grad_norm": 2.2749791145324707,
      "learning_rate": 8.231807704972012e-08,
      "loss": 0.0608,
      "step": 3370
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.9533,
      "eval_loss": 0.13899274170398712,
      "eval_runtime": 58.7053,
      "eval_samples_per_second": 170.342,
      "eval_steps_per_second": 8.517,
      "step": 3375
    },
    {
      "epoch": 3.0,
      "step": 3375,
      "total_flos": 2.115330001956864e+19,
      "train_loss": 0.1813771443190398,
      "train_runtime": 3365.7127,
      "train_samples_per_second": 80.221,
      "train_steps_per_second": 1.003
    }
  ],
  "logging_steps": 10,
  "max_steps": 3375,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "total_flos": 2.115330001956864e+19,
  "train_batch_size": 20,
  "trial_name": null,
  "trial_params": null
}
