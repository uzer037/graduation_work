{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDaB4JBJdjTv"
      },
      "source": [
        "# Оригинальная модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDsMEzl3Aq5h"
      },
      "outputs": [],
      "source": [
        "# %pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoImageProcessor, SwinForImageClassification\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVgc-goUgL9a",
        "outputId": "b31c37c3-1798-4c0d-d20c-6afa95586318"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "# @title Среда\n",
        "import torch\n",
        "from transformers import AutoImageProcessor\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUuX1LWCdIGS",
        "outputId": "b9a23f3e-63c5-4447-8d11-65cba2b7174b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\bes-s\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\datasets\\table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n"
          ]
        }
      ],
      "source": [
        "# @title Данные\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"mvkvc/artifact-100k\") # subset of ArtiFact labled with \"ai\" and \"real\" tags\n",
        "\n",
        "dataset = dataset.with_format(type=\"torch\", device=device)\n",
        "\n",
        "image_processor = AutoImageProcessor.from_pretrained(\"umm-maybe/AI-image-detector\")\n",
        "\n",
        "batch_size = 40\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=dataset[\"train\"], batch_size=batch_size,  num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=dataset[\"test\"], batch_size=batch_size, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'height': 230, 'width': 230}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image_processor.do_normalize = True\n",
        "image_processor.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F45gzl72Apkb",
        "outputId": "7a6ee7f4-5718-49c7-e98c-a0794b10cea6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at checkpoints/1-1/ were not used when initializing SwinForImageClassification: ['classifier.1.bias', 'classifier.1.weight']\n",
            "- This IS expected if you are initializing SwinForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing SwinForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of SwinForImageClassification were not initialized from the model checkpoint at checkpoints/1-1/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Модель загружена\n",
            "SwinForImageClassification(\n",
            "  (swin): SwinModel(\n",
            "    (embeddings): SwinEmbeddings(\n",
            "      (patch_embeddings): SwinPatchEmbeddings(\n",
            "        (projection): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
            "      )\n",
            "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (encoder): SwinEncoder(\n",
            "      (layers): ModuleList(\n",
            "        (0): SwinStage(\n",
            "          (blocks): ModuleList(\n",
            "            (0-1): 2 x SwinLayer(\n",
            "              (layernorm_before): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (attention): SwinAttention(\n",
            "                (self): SwinSelfAttention(\n",
            "                  (query): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (key): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (value): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "                (output): SwinSelfOutput(\n",
            "                  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (drop_path): SwinDropPath(p=0.1)\n",
            "              (layernorm_after): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (intermediate): SwinIntermediate(\n",
            "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
            "                (intermediate_act_fn): GELUActivation()\n",
            "              )\n",
            "              (output): SwinOutput(\n",
            "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
            "                (dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (downsample): SwinPatchMerging(\n",
            "            (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
            "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "        (1): SwinStage(\n",
            "          (blocks): ModuleList(\n",
            "            (0-1): 2 x SwinLayer(\n",
            "              (layernorm_before): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (attention): SwinAttention(\n",
            "                (self): SwinSelfAttention(\n",
            "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
            "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
            "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
            "                  (dropout): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "                (output): SwinSelfOutput(\n",
            "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
            "                  (dropout): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (drop_path): SwinDropPath(p=0.1)\n",
            "              (layernorm_after): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (intermediate): SwinIntermediate(\n",
            "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
            "                (intermediate_act_fn): GELUActivation()\n",
            "              )\n",
            "              (output): SwinOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
            "                (dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (downsample): SwinPatchMerging(\n",
            "            (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
            "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "        (2): SwinStage(\n",
            "          (blocks): ModuleList(\n",
            "            (0-17): 18 x SwinLayer(\n",
            "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "              (attention): SwinAttention(\n",
            "                (self): SwinSelfAttention(\n",
            "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
            "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
            "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
            "                  (dropout): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "                (output): SwinSelfOutput(\n",
            "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "                  (dropout): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (drop_path): SwinDropPath(p=0.1)\n",
            "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "              (intermediate): SwinIntermediate(\n",
            "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                (intermediate_act_fn): GELUActivation()\n",
            "              )\n",
            "              (output): SwinOutput(\n",
            "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                (dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (downsample): SwinPatchMerging(\n",
            "            (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
            "            (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "        (3): SwinStage(\n",
            "          (blocks): ModuleList(\n",
            "            (0-1): 2 x SwinLayer(\n",
            "              (layernorm_before): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (attention): SwinAttention(\n",
            "                (self): SwinSelfAttention(\n",
            "                  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                  (dropout): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "                (output): SwinSelfOutput(\n",
            "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                  (dropout): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (drop_path): SwinDropPath(p=0.1)\n",
            "              (layernorm_after): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (intermediate): SwinIntermediate(\n",
            "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                (intermediate_act_fn): GELUActivation()\n",
            "              )\n",
            "              (output): SwinOutput(\n",
            "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    (pooler): AdaptiveAvgPool1d(output_size=1)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Sigmoid()\n",
            "    (1): Linear(in_features=1024, out_features=2, bias=True)\n",
            "    (2): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# @title Модель\n",
        "\n",
        "PATH = 'checkpoints/1-1/'\n",
        "model = SwinForImageClassification.from_pretrained(PATH, local_files_only=True)\n",
        "model.classifier = torch.nn.Sequential(\n",
        "    # torch.nn.Linear(in_features=768, out_features=2, bias=True),\n",
        "    torch.nn.Sigmoid(),\n",
        "    torch.nn.Linear(in_features=1024, out_features=2, bias=True),\n",
        "    torch.nn.Sigmoid()\n",
        "    )\n",
        "model.to(device)\n",
        "print(\"Модель загружена\")\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFLGMwlD_DO-",
        "outputId": "3c48023f-a259-4ae6-da0e-82fc08624242"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoImageProcessor, SwinForImageClassification\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "\n",
        "# dataset = load_dataset(\"huggingface/cats-image\")\n",
        "\n",
        "# image_processor = AutoImageProcessor.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
        "# model = SwinForImageClassification.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED-i9QQKdolK"
      },
      "source": [
        "# Finetune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kVBRL2TC02XA"
      },
      "outputs": [],
      "source": [
        "# @title EarlyStopping\n",
        "class EarlyStopping:\n",
        "    def __init__(self, value=0.7, patience=5):\n",
        "        self.value = value\n",
        "        self.patience = patience\n",
        "        self.wait = 0\n",
        "        self.stopped_epoch = 0\n",
        "        self.best = -float('inf')\n",
        "        self._improved_this_step = False\n",
        "\n",
        "    def improved_this_step(self) -> bool:\n",
        "        return self._improved_this_step\n",
        "\n",
        "    def _condition(self, value) -> bool:\n",
        "        return False\n",
        "\n",
        "    def __call__(self, epoch, value) -> bool:\n",
        "        self._improved_this_step = False\n",
        "        if self._condition(value):\n",
        "            self.best = value\n",
        "            self.wait = 0\n",
        "            self._improved_this_step = True\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                self.stopped_epoch = epoch\n",
        "                print(f\"Epoch {epoch}: early stopping\")\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "class EarlyStoppingByAccuracy(EarlyStopping):\n",
        "    def _condition(self, value) -> bool:\n",
        "        return value > self.best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for batch_ind, data in enumerate(test_loader):\n",
        "#      inputs, targets = data['image'], data['label']\n",
        "#      targets = torch.nn.functional.one_hot(targets, num_classes = 2).to(torch.float)\n",
        "#      print (targets)\n",
        "#      break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "asdzdwXSeOb5",
        "outputId": "5a6a6b3f-a482-4299-d714-fb648e7f97f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch #1/1000..."
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 128}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done in 13.283333 minutes\n",
            "epoch #2/1000...done in 12.983333 minutes\n",
            "epoch #3/1000...done in 12.983333 minutes\n",
            "epoch #4/1000...done in 12.983333 minutes\n",
            "epoch #5/1000...done in 12.983333 minutes\n",
            "epoch #6/1000...Epoch 5: early stopping\n",
            "done in 12.983333 minutes\n",
            "done training.\n"
          ]
        }
      ],
      "source": [
        "# @title Training loop { display-mode: \"form\" }\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# loss_func = torch.nn.L1Loss()\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.8)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.08)\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5, verbose=True)\n",
        "\n",
        "import torch\n",
        "import datetime\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "\n",
        "def calculate_epoch(writer: SummaryWriter, epoch, is_train=False):\n",
        "  data_loader = train_loader if is_train else test_loader\n",
        "  if is_train:\n",
        "    model.train(True)\n",
        "  else:\n",
        "    model.eval()\n",
        "\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  avg_loss = 0\n",
        "  batches = 0\n",
        "\n",
        "  for batch_ind, data in enumerate(data_loader):\n",
        "    inputs, targets = data['image'], data['label']\n",
        "    inputs = inputs.permute(0,3,1,2)\n",
        "    inputs, targets = inputs.to(torch.float32).to(device), targets.to(torch.long).to(device)\n",
        "    # targets = targets.unsqueeze(1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output = model(inputs).logits\n",
        "    total += len(output)\n",
        "    correct += (output.argmax() == targets).float().sum()\n",
        "\n",
        "    targets = torch.nn.functional.one_hot(targets, num_classes = 2).to(torch.float)\n",
        "    loss = loss_func(output, targets)\n",
        "    \n",
        "    if is_train:\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    avg_loss += loss.item()\n",
        "    batches += 1\n",
        "\n",
        "  avg_loss = avg_loss / batches\n",
        "  accuracy = correct/total\n",
        "\n",
        "  stage = \"train\" if is_train else \"test\"\n",
        "  writer.add_scalar(f\"loss/{stage}\", avg_loss, epoch)\n",
        "  writer.add_scalar(f\"accuracy/{stage}\", accuracy, epoch * total)\n",
        "  if stage == \"train\":\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    writer.add_scalar('Learning Rate', current_lr, epoch)\n",
        "  return {\"acc\": accuracy, \"avg_loss\" : avg_loss}\n",
        "\n",
        "\n",
        "def train_loop():\n",
        "  time_now = datetime.datetime.now()\n",
        "  layout = {\n",
        "      \"Model statistics\" : {\n",
        "      \"loss\": [\"Multiline\", [\"loss/train\", \"loss/test\"]],\n",
        "      \"accuracy\": [\"Multiline\", [\"accuracy/train\", \"accuracy/test\"]],\n",
        "      \"learning_rate\": [\"Scalar\", \"Learning Rate\"]\n",
        "      }\n",
        "  }\n",
        "  writer = SummaryWriter(log_dir=time_now.strftime(\"runs/SWIN/run %d-%m %H_%M_%S\"))\n",
        "  writer.add_custom_scalars(layout)\n",
        "\n",
        "  early_stop = EarlyStoppingByAccuracy()\n",
        "  epoch_limit = 1000\n",
        "  epoch = 0\n",
        "  is_training = True\n",
        "  while is_training and epoch < epoch_limit:\n",
        "    print(f\"epoch #{epoch+1}/{epoch_limit}...\", end = \"\")\n",
        "    time_start = datetime.datetime.now()\n",
        "    calculate_epoch(writer, epoch, is_train=True)\n",
        "\n",
        "    report = None\n",
        "    with torch.no_grad():\n",
        "      report = calculate_epoch(writer, epoch, is_train=False)\n",
        "      scheduler.step(report[\"avg_loss\"])\n",
        "    if early_stop(epoch, report[\"acc\"]):\n",
        "      is_training = False\n",
        "\n",
        "\n",
        "    if early_stop.improved_this_step():\n",
        "      model.save_pretrained(f\"./checkpoints/{epoch}\", safe_serialization=False) # saves as SafeTensors\n",
        "\n",
        "      \n",
        "    time_end = datetime.datetime.now()\n",
        "    deltatime = time_end - time_start\n",
        "    print(f\"done in {deltatime.seconds/60:4f} minutes\")\n",
        "    writer.flush()\n",
        "    epoch += 1\n",
        "\n",
        "  writer.close()\n",
        "  print('done training.')\n",
        "\n",
        "train_loop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save_pretrained(\"./checkpoints\", safe_serialization=True) # saves as SafeTensors"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "TDaB4JBJdjTv",
        "ED-i9QQKdolK"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
